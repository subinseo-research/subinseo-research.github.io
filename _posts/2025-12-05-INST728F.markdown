---
layout: post
title:  "MindFlex AI"
date:   2025-12-5 18:05:55 +0300
image:  post_11_0.png
tags:   Human-Centered System    
---

 <h1>Overview</h1>
 <br><br>

<div style="text-align: justify;">
<h3>üîçProject Context</h3>
<p>MindFlex AI is an adaotive information system powered by gnerative AI models, which can provide personalized information and adjust their UI based on users' queries.  Therefore, it is an intelligent system in which AI reads the emotional state of the human user and responds flexibly accordingly.
<br><br>
Jensen Huang, the CEO of NVIDIA,<a href="https://www.instagram.com/reel/DR4QwDcDIVd/" target="_blank"> describes AI as the easiest tool</a> in human history and one that can help realize equity. I agree with this to some extent. More precisely, I believe that while AI is not yet such a tool, it holds that potential. Users can interact with AI through a simple chat in their preferred language, which has contributed to the rapid popularity of models like ChatGPT and Gemini. However, despite the wide diversity in users' backgrounds and contexts, these general-purpose AI models provide the same environment to all users. As a result, the quality of AI responses can vary significantly depending on users' literacy and metacognitive abilities. To address this disparity, I propose an adaptive system that reflects users' context with the UI components. 
<br><br>
This project was proposed in the course INST728F in 2025 Fall semester, led by Professor Tom Brinck at the University of Maryland, and the primary focus was to implement these ideas through UX/UI design.
</p> 
</div>

---
<div style="text-align: justify;">
    <br>
    <h3>‚ùìProblem</h3>
    Most AI systems have been optimized primarily for <u>accurcay-related metrics</u> such as F-meaurse. However, real human information-seeking behavior is far more influenced by the context in which the system is used and by psychological factors such as cognitive load and trust calibration. 
    <br><br>
    Misuse of general-purpose AI can lead to truly dangerous consequences. <a href="https://www.youtube.com/watch?v=jBnJlwcnOBI" target="_blank"> In August 2025, there was a case in which a student died by suicide after interacting with ChatGPT </a>. This incident occurred, in part, because generative AIs like ChatGPT are generally designed to respond supportively to users regardless of the purpose of use. However, this issue cannot simply be dismissed as an individual's poor judgment. To design safer and more equitable information systems, we must confront and address these challenges directly.
    <br><br>
    Therefore, this project explores what kinds of design are needed to prevent misuse and build a safer information system.
    <br><br>
    <div class="notice--blue">
    "How can I reduce users' <strong>cognitive, emotional, and informational misuse</strong> in general-purpose AI?"
    </div>
    <br>
</div>

---
<h3>üéØDesogm Goals and Final Designs</h3>

<br>
<br>
<strong>Design Goal 1Ô∏è‚É£: Support for various modes tailored to users needs</strong>
![Mainstream GIF](/images/Post_11/MainStream.gif)
* Support for five distinct modes:<br> 
            research, development, creation, business, and counseling
* The settings page allows for more specific customization 
            : preferred sources, citation style, level of alerts 
<br>

![Mainstream2](/images/Post_11/Mainstream2.gif)
Users can choose the best-fit mode through conversation
<br>

![Mainstream3](/images/Post_11/Mainstream3.gif)
If a user accidentally uses the wrong mode during a conversation, the system detects it and suggests switching to the appropriate mode.

<br>
<strong>Design Goal 2Ô∏è‚É£: Guide users to identify their specific information needs</strong>

![Function1 GIF](/images/Post_11/Function1.gif)
If a user submits an overly broad prompt, the system asks follow-up questions to better serve the user's ultimate needs. However, <u>to prevent users from feeling burdened by answering these questions, the design allows for quick and easy responses through clickable options.</u>

<br>
<strong>Design Goal 3Ô∏è‚É£: Build user trust in sources</strong>

![Function2 GIF](/images/Post_11/Function2.gif)
Many recent generative AI services already provide source citations. However, I added a citation type selection button so that users can easily copy and view citations at once, while also presenting an overall credibility message to encourage users' metacognitive judgment.
<br><br>
For credibility message, 
* If the number of sources in a generated response is between 0 and 2, the system issues a warning that the content may be dangerous üî¥.
* If the number of sources is between 3 and 5, the system issues a warning that the information may be insufficient üü°.
* If the number of sources is 6 or more, the system indicates that the information is safe üü¢.

<br>
<strong>Design Goal 4Ô∏è‚É£: Detect Overtrust</strong>

![Function3 GIF](/images/Post_11/Function3.gif)

When the system detects that a user overly relies on AI, it intentionally disrupts the flow by displaying a warning message instead of responding in the chat window.


---
 <h1>Discovery</h1>
 <br><br>

<div align="center">
  <img src="/images/Post_11/interview.png" alt="interview"> <br>
  4 Interviews 
</div>
<br>


<h3>üíÅüèª‚Äç‚ôÄÔ∏èUser study</h3>

Who are we designing for?
Users want to...
"

---
 <h1>User Behavior Flow</h1>
 <br><br>

<div align="center">
  <img src="/images/Post_11/flow.png" alt="flow"> 
</div>




<br>
---
 <h1>üí°Ideation</h1>
 <br><br>

<h3>Low-fidelity Design Sketch</h3>
I created the initial design by drawing it by hand.

![Low-fidelity_wireframe](/images/Post_11/Low-fidelity_wireframe.jpg)


<br>
<h3>Wireframe</h3>
Based on the sketch, we created wireframes, which helped ensure the designs aligned with the intended design goals.

![Highfidelity_wireframe](/images/Post_11/High-fidelity_wireframe.png)

<br>
<h3>Design Indentity</h3>
<div align="center">
  <img src="/images/Post_11/UI_Color.png" alt="UI_Color" width="80%">
</div>
Through aesthetic design, I also wanted to highlight the system‚Äôs core identity‚Äîoffering a variety of selectable modes. To achieve this, the overall system design was kept simple, using gray tones, black text, and a white background. However, I used distinct and vibrant colors for each mode to visually differentiate them. Each color was carefully chosen to match the concept of its corresponding mode:
* Research ‚Äî Soft Study Blue (#DCEBFF)üîµ
* Tech ‚Äî Mint Logic Green (#E3FCEF)üü¢
* Creative ‚Äî Pastel Neon Purple (#F4E8FF)üü£
* Business ‚Äî Warm Office Orange (#FFE9CC)üü†
* Counselor ‚Äî Soft Emotional Pink (#FFE5F0)üíó 
<br>
Lastly, the logo combines these five colors into a single wave, forming the shape of a brain to symbolize the mind.

<br>
---
<h1>üîÅIteration</h1>
<br>



---
<h1>‚ú®Reflection</h1>
<br>
<div style="display: flex; gap: 20px; align-items: flex-start;">


  <div style="flex: 1; text-align: center;">
    <img src="/images/Post_11/AI_use.gif" 
         alt="Project Image" 
         style="width: 100%; max-width: 450px; border-radius: 8px;">
  </div>

  <div style="flex: 2; text-align: justify;">
    <h3>Use of Artificial Intelligence</h3>
    <p>
      The goal of this course was to successfully complete a project by appropriately utilizing AI. 
      For this project, I used GPT Pro, Figma Pro, and Whyser (an interview research tool). 
      In chronological order, I began by conducting interviews with four individuals to explore the potential significance 
      and necessity of the project. During this process, I used Whyser and ChatGPT to generate appropriate interview 
      questions and summarize the interview content.
      Next, I collaborated with ChatGPT for brainstorming to sketch out the initial design structure. 
      Based on this, I created wireframes using Figma. 
      While doing so, I also used ChatGPT to develop concrete user scenarios to include in the wireframes.
      <br><br>
      Finally, I used Figma AI‚Äîone of the main tools in this project‚Äîto implement dynamic interactions. 
      Since the goal of my project was to simulate a generative AI system, I aimed to produce higher-fidelity, dynamic 
      wireframes using Figma‚Äôs built-in AI capabilities.
    </p>

    <h3>Challenges</h3>
    <p>
      Since it was my first time actually designing a UI, the process felt unfamiliar, and I was not confident using Figma. 
      As a result, I relied heavily on the support of various AI models throughout the project. 
      Although Figma AI is very useful, when designing multi-layered pages, the request can sometimes be applied to the 
      wrong page if the specific page is not clearly mentioned. 
      (This can happen even when the request is made within a certain screen, but the changes are reflected in a different one.)
    </p>
  </div>
</div>

<h3>Future Work</h3>
My personal research goal is to design and develop human-centered systems. In most research, this mostly involves analyzing user behavior or fine-tuning algorithms. However, in order to experiment with systems withh human subjects through mock systems, I believe it is essential to also have experience in UX/UI design and implementation. That is why I undertook this project. In the future, I will focus on improving algorithms and enhancing the quality of information. Once the design goals can be implemented algorithmically, I plan to use the experience gained from this project to guide the development of more effective system designs.


<br><br><br>